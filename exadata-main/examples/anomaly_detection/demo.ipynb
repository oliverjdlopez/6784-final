{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6b890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c0318b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './388.parquet'\n",
    "data = pd.read_parquet(file_name)\n",
    "data.dropna(inplace = True)\n",
    "data.reset_index(drop=True, inplace = True)\n",
    "names = list(data)\n",
    "size = 0.8\n",
    "test = data[int(data.shape[0]*size):]\n",
    "data = data[:int(data.shape[0]*size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735b84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39035/39035 [==============================] - 15s 379us/step - loss: 0.0220\n",
      "Epoch 2/5\n",
      "39035/39035 [==============================] - 13s 330us/step - loss: 0.0087\n",
      "Epoch 3/5\n",
      "39035/39035 [==============================] - 13s 340us/step - loss: 0.0069\n",
      "Epoch 4/5\n",
      "39035/39035 [==============================] - 14s 356us/step - loss: 0.0064\n",
      "Epoch 5/5\n",
      "39035/39035 [==============================] - 14s 355us/step - loss: 0.0056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20023a80bc88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "#general_variables\n",
    "timestep = 20\n",
    "\n",
    "\n",
    "\n",
    "#FIT scaler on train data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaler = StandardScaler()\n",
    "scaler = scaler.fit(data.drop('timestamp', axis = 1).values)\n",
    "\n",
    "\n",
    "\n",
    "diffs = dict(data['timestamp'].diff().apply(lambda x: x/np.timedelta64(1, 'm')).fillna(0).astype('int64'))\n",
    "large_diffs = {i:diffs[i] for i in diffs.keys() if diffs[i] > 15}\n",
    "\n",
    "\n",
    "diffs_test = dict(test['timestamp'].diff().apply(lambda x: x/np.timedelta64(1, 'm')).fillna(0).astype('int64'))\n",
    "large_diffs_test = {i:diffs_test[i] for i in diffs_test.keys() if diffs_test[i] > 15}\n",
    "\n",
    "\n",
    "lk = list(large_diffs.keys())\n",
    "lk.sort()\n",
    "lk_test = list(large_diffs_test.keys())\n",
    "lk_test.sort()\n",
    "\n",
    "l_mod = [0] + lk + [data.shape[0] +1]\n",
    "l_mod_test = [0] + lk_test + [test.shape[0] +1]\n",
    "\n",
    "list_of_dfs = [data.iloc[l_mod[n]:l_mod[n+1]] for n in range(len(l_mod)-1)]\n",
    "list_of_dfs = [i for i in list_of_dfs if i.shape[0] > timestep]\n",
    "\n",
    "list_of_dfs_test = [test.iloc[l_mod_test[n]:l_mod_test[n+1]] for n in range(len(l_mod_test)-1)]\n",
    "list_of_dfs_test = [i for i in list_of_dfs_test if i.shape[0] > timestep]\n",
    "\n",
    "\n",
    "lenghts = {i:list_of_dfs[i].shape[0] for i in range(len(list_of_dfs))}\n",
    "lenghts_test = {i:list_of_dfs_test[i].shape[0] for i in range(len(list_of_dfs_test))}\n",
    "\n",
    "data_gens = []\n",
    "for df in list_of_dfs:\n",
    "    normalized = scaler.transform(df.drop('timestamp', axis = 1).values)\n",
    "    data_gens.append(TimeseriesGenerator(normalized, normalized,\n",
    "                               length=timestep, sampling_rate=1,\n",
    "                               batch_size=1000))\n",
    "\n",
    "\n",
    "data_gens_test = []\n",
    "for df in list_of_dfs_test:\n",
    "    normalized = scaler.transform(df.drop('timestamp', axis = 1).values)\n",
    "    data_gens_test.append(TimeseriesGenerator(normalized, normalized,\n",
    "                               length=timestep, sampling_rate=1,\n",
    "                               batch_size=1000))  \n",
    "\n",
    "train = np.vstack([np.array(i[0][0][:]) for i in data_gens])\n",
    "test = np.vstack([np.array(i[0][0][:]) for i in data_gens_test])\n",
    "\n",
    "Labels = test[:,-1,-1]\n",
    "\n",
    "train = train[:,:,:-1]\n",
    "test = test[:,:,:-1]\n",
    "\n",
    "y_train = train[:, -1, :]\n",
    "y_test = test[:, -1, :]\n",
    "\n",
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(timestep, X.shape[2]))\n",
    "    L1 = LSTM(16, activation = 'selu', return_sequences = True)(inputs)\n",
    "    L2 = LSTM(16, activation = 'selu', return_sequences = True)(L1)\n",
    "    L3 = LSTM(8, activation = 'selu', return_sequences = False)(L2)\n",
    "    L4 = Dense(16, activation = 'selu')(L3)\n",
    "    L5 = Dense(16, activation = 'selu')(L4)\n",
    "    output = Dense(X.shape[2])(L5)\n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model\n",
    "\n",
    "model = autoencoder_model(train)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.MeanSquaredError())\n",
    "model.fit(train, y_train,batch_size = 128, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f9cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 450us/step\n",
      "0.6266266266266266\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test, verbose = True)\n",
    "predictions_df = pd.DataFrame(predictions, columns=names[1:-1])\n",
    "val_df = pd.DataFrame(y_test, columns=names[1:-1])\n",
    "normal_diff = val_df - predictions_df\n",
    "\n",
    "normal_diff['Total absolute reproduction error'] = normal_diff.abs().sum(axis=1)\n",
    "M = max(normal_diff['Total absolute reproduction error'])\n",
    "normal_diff['real_label'] = Labels\n",
    "\n",
    "normal_diff['real_label'] = normal_diff['real_label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "normal_diff['Softmax'] = normal_diff['Total absolute reproduction error'].apply(lambda x: x/M) \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# calculate scores\n",
    "auc = roc_auc_score(normal_diff['real_label'], normal_diff['Softmax'])\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
